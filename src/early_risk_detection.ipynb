{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ttODwCFd8K0Q"
   },
   "source": [
    "# Access the MentalRiskEs data and interact with the server\n",
    "\n",
    "This notebook has been developed by the [SINAI](https://sinai.ujaen.es/) research group for its usage in the [MentalRiskES](https://sites.google.com/view/mentalriskes/) evaluation campaign at IberLEF 2023.\n",
    "\n",
    "**NOTE 1**: Please visit the [MentalRiskES competition website](https://sites.google.com/view/mentalriskes/evaluation) to read the instructions about how to download the data and interact with the server to send the predictions of your system.\n",
    "\n",
    "**NOTE 2**: Along the code, please replace \"URL\" by the URL server and \"TOKEN\" by your personal token.\n",
    "\n",
    "Remember this is a support to help you to develop your own system of communication with our server. We recommend you to download it as a Python script instead of working directly on colab and adapt the code to your needs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2DJN0pXx8W3-"
   },
   "source": [
    "# Install CodeCarbon package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdvPWyc6x9cV"
   },
   "outputs": [],
   "source": [
    "# -- Install libraries\n",
    "!pip install llvmlite --ignore-installed\n",
    "!pip install pycaret -U\n",
    "!pip install empath\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install codecarbon\n",
    "!pip install emoji\n",
    "!pip install beautifulsoup4\n",
    "!pip install emosent-py\n",
    "!pip install googletrans==3.1.0a0 -U\n",
    "!pip install empath\n",
    "!pip install spacy\n",
    "!pip install typer==0.6.1\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install syntok\n",
    "!pip install NRCLex\n",
    "!pip install readability\n",
    "!pip install vaderSentiment\n",
    "!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
    "\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dqyN-7TcXbL8"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sqih7m6tN4MT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12040 | INFO | NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from emosent           import get_emoji_sentiment_rank\n",
    "from empath import Empath\n",
    "import requests, zipfile, io\n",
    "from typing import List, Dict\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from googletrans import Translator\n",
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "from   nltk.stem                       import WordNetLemmatizer\n",
    "from   tqdm                            import tqdm\n",
    "from   nrclex                          import NRCLex\n",
    "from   textblob                        import TextBlob\n",
    "import transformers\n",
    "import syntok.segmenter                as segmenter\n",
    "import matplotlib.pyplot               as plt\n",
    "import pandas                          as pd\n",
    "import numpy                           as np\n",
    "import readability\n",
    "import lightgbm\n",
    "import spacy\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import emosent\n",
    "import time\n",
    "import nltk\n",
    "import gc\n",
    "import re\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "emoji_data_df = pd.read_csv('emoji_data_df.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CHGGrr3GXdIb"
   },
   "source": [
    "# Endpoints\n",
    "These URL addresses are necessary for the connection to the server. \n",
    "\n",
    "**IMPORTANT:** Replace \"URL\" by the URL server and \"TOKEN\" by your user token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AdQPl8lbOKsg"
   },
   "outputs": [],
   "source": [
    "URL = \"http://s3-ceatic.ujaen.es:8036\" \n",
    "TOKEN = \"bb8e33a437c180e88c4b79f8aed93dc00df4f6a9\" \n",
    "\n",
    "# Download endpoints\n",
    "ENDPOINT_DOWNLOAD_MESSAGES_TRIAL = URL+\"{TASK}/download_trial/{TOKEN}\"\n",
    "ENDPOINT_DOWNLOAD_GOLD_TRIAL = URL+\"{SUBTASK}/download_trial/{TOKEN}\"\n",
    "ENDPOINT_DOWNLOAD_MESSAGES_TRAIN = URL+\"{TASK}/download_train/{TOKEN}\"\n",
    "ENDPOINT_DOWNLOAD_GOLD_TRAIN = URL+\"{SUBTASK}/download_train/{TOKEN}\"\n",
    "\n",
    "# Trial endpoints\n",
    "ENDPOINT_GET_MESSAGES_TRIAL = URL+\"/{TASK}/getmessages_trial/{TOKEN}\"\n",
    "ENDPOINT_SUBMIT_DECISIONS_TRIAL = URL+\"/{SUBTASK}/submit_trial/{TOKEN}/{RUN}\"\n",
    "\n",
    "# Test endpoints\n",
    "ENDPOINT_GET_MESSAGES = URL+\"/{TASK}/getmessages/{TOKEN}\"\n",
    "ENDPOINT_SUBMIT_DECISIONS = URL+\"/{SUBTASK}/submit/{TOKEN}/{RUN}\"\n",
    "\n",
    "# Emissions Tracker Config\n",
    "config = {\n",
    "    \"save_to_file\": True,\n",
    "    \"log_level\": \"DEBUG\",\n",
    "    \"tracking_mode\": \"process\",\n",
    "    \"output_dir\": \".\", \n",
    "}\n",
    "\n",
    "# -- Variables\n",
    "SUB_POL_LIST   = ['fe_subjectivity', 'fe_polarity']\n",
    "SENTIMENT_LIST = ['fe_roberta_base_sentiment_negative', 'fe_roberta_base_sentiment_neutral', 'fe_roberta_base_sentiment_positive', 'fe_vader_positive_sentiment', 'fe_vader_neutral_sentiment', 'fe_vader_negative_sentiment']\n",
    "EMOTION_LIST   = ['fe_distilbert_emotion_optimism', 'fe_distilbert_emotion_joy', 'fe_distilbert_emotion_sadness', 'fe_distilbert_emotion_anger', 'fe_nrclex_emotion_fear', 'fe_nrclex_emotion_anger', 'fe_nrclex_emotion_anticip', 'fe_nrclex_emotion_trust', 'fe_nrclex_emotion_surprise', 'fe_nrclex_emotion_positive', 'fe_nrclex_emotion_negative', 'fe_nrclex_emotion_sadness', 'fe_nrclex_emotion_disgust', 'fe_nrclex_emotion_joy']\n",
    "TOXIC_LIST     = ['fe_toxic', 'fe_severe_toxic', 'fe_obscene', 'fe_threat', 'fe_insult', 'fe_identity_hate']\n",
    "\n",
    "# -- Empath features\n",
    "EMOTE_LIST = list(set(['hate', 'envy', 'health', 'nervousness', 'weakness', 'horror', 'suffering', 'kill', 'fear', 'friends', 'sexual', 'body', 'family',\n",
    "                       'irritability','violence','sadness','disgust','exasperation','emotional','anger','poor','pain','timidity','cheerfulness', 'night', 'college', 'sports', 'neglect',\n",
    "                       'medical_emergency','rage','alcohol','positive_emotion','negative_emotion','ugliness','weapon','shame','torment','office','help',\n",
    "                       'sleep', 'money', 'school', 'home', 'hygiene', 'phone', 'work', 'appereance', 'optimism', 'youth', 'joy', 'valuable', 'swearing_terms',\n",
    "                       'disappointment', 'children', 'contentment', 'music', 'musical', 'deception', 'blue_collar_job', 'clothing', 'white_collar_job', 'exercise']))\n",
    "# -- POS features\n",
    "POS_FEATURES_LIST = ['fe_pos_adjs', 'fe_pos_advs', 'fe_pos_verbs', 'fe_pos_nouns', 'fe_pos_past_tense_verbs']\n",
    "\n",
    "# -- Subjectivity and polarity - ROUND\n",
    "SUB_POL_LIST = ['fe_subjectivity', 'fe_polarity']\n",
    "\n",
    "# -- Text features \n",
    "TEXT_FEATURES_LIST = ['fe_punct_signs', 'fe_number_uppercase_words', 'fe_num_first_person_pronouns', 'fe_num_antidepressants', 'fe_num_words', 'fe_num_sentences', 'fe_num_paragraphs', 'fe_num_long_words', 'fe_num_complex_words', 'fe_num_emojis_emoticons', 'fe_num_negations', 'fe_num_pos_emojis', 'fe_num_neut_emojis', 'fe_num_neg_emojis']\n",
    "\n",
    "# -- Date features\n",
    "DATE_FEATURES = ['fe_posted_early_morning', 'fe_posted_morning', 'fe_posted_afternoon', 'fe_posted_night', 'fe_posted_first_season', 'fe_posted_second_season', 'fe_posted_third_season', 'fe_posted_fourth_season']\n",
    "\n",
    "# -- Readability index\n",
    "READABILITY_LIST = ['fe_kincaid_readability_index', 'fe_ari_readability_index', 'fe_coleman_readability_index', 'fe_flesch_readability_index', 'fe_gunning_fog_readability_index', 'fe_lix_readability_index', 'fe_smog_index']\n",
    "\n",
    "# -- Depression terms\n",
    "TERMS_LIST = ['fe_num_anhedonia_terms', 'fe_num_concentration_terms', 'fe_num_eating_terms', 'fe_num_fatigue_terms', 'fe_num_mood_terms', 'fe_num_psychomotor_terms', 'fe_num_self-esteem_terms', 'fe_num_self-harm_terms', 'fe_num_sleep_disorder_terms', 'fe_num_panic_attacks_terms']\n",
    "\n",
    "# -- Transformers - Sentiment - ROUND\n",
    "SENTIMENT_LIST = ['fe_roberta_base_sentiment_negative', 'fe_roberta_base_sentiment_neutral', 'fe_roberta_base_sentiment_positive', 'fe_vader_positive_sentiment', 'fe_vader_neutral_sentiment', 'fe_vader_negative_sentiment']\n",
    "\n",
    "# -- Transformers - Emotion - ROUND\n",
    "EMOTION_LIST = ['fe_distilbert_emotion_optimism', 'fe_distilbert_emotion_joy', 'fe_distilbert_emotion_sadness', 'fe_distilbert_emotion_anger', 'fe_nrclex_emotion_fear', 'fe_nrclex_emotion_anger', 'fe_nrclex_emotion_anticip', 'fe_nrclex_emotion_trust', 'fe_nrclex_emotion_surprise', 'fe_nrclex_emotion_positive', 'fe_nrclex_emotion_negative', 'fe_nrclex_emotion_sadness', 'fe_nrclex_emotion_disgust', 'fe_nrclex_emotion_joy']\n",
    "\n",
    "# -- Transformers - Toxic - ROUND\n",
    "TOXIC_LIST = ['fe_toxic', 'fe_severe_toxic', 'fe_obscene', 'fe_threat', 'fe_insult', 'fe_identity_hate']\n",
    "\n",
    "ORIGINAL_SUM_FEATURE_COLS = EMOTE_LIST + POS_FEATURES_LIST + TEXT_FEATURES_LIST + DATE_FEATURES + TERMS_LIST\n",
    "ORIGINAL_MEAN_FEATURE_COLS= SENTIMENT_LIST + EMOTION_LIST + TOXIC_LIST + SUB_POL_LIST\n",
    "ORIGINAL_REDABILITY_LIST  = READABILITY_LIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}-latest\"\n",
    "pipe_sentiment = transformers.pipeline(\"text-classification\", model=MODEL, return_all_scores=True, max_length=511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_emotion = transformers.pipeline(\"text-classification\", model=\"cardiffnlp/roberta-base-emotion\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_toxicity = transformers.pipeline(model=\"unitary/toxic-bert\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load models and features per model\n",
    "import pickle\n",
    "with open('./lightgbm_2023_05_13_google_translator.pkl', 'rb') as f:\n",
    "    lightgbm_model = pickle.load(f)\n",
    "\n",
    "sum_feature_cols_lightgbm  = [feature for feature in ORIGINAL_SUM_FEATURE_COLS if feature in lightgbm_model.feature_name_]\n",
    "mean_feature_cols_lightgbm = [feature + '_mean' for feature in ORIGINAL_MEAN_FEATURE_COLS if feature + '_mean' in lightgbm_model.feature_name_]\n",
    "readability_list_lightgbm  = [feature for feature in ORIGINAL_REDABILITY_LIST if feature in lightgbm_model.feature_name_]\n",
    "features_lightgbm          = [sum_feature_cols_lightgbm, mean_feature_cols_lightgbm, readability_list_lightgbm]\n",
    "\n",
    "with open('./rf_2023_05_13_google_translator.pkl', 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "sum_feature_cols_rf  = [feature for feature in ORIGINAL_SUM_FEATURE_COLS if feature in rf_model.feature_names_in_]\n",
    "mean_feature_cols_rf = [feature + '_mean' for feature in ORIGINAL_MEAN_FEATURE_COLS if feature + '_mean' in rf_model.feature_names_in_]\n",
    "readability_list_rf  = [feature for feature in ORIGINAL_REDABILITY_LIST if feature in rf_model.feature_names_in_]\n",
    "features_rf          = [sum_feature_cols_rf, mean_feature_cols_rf, readability_list_rf]\n",
    "\n",
    "with open('./lr_2023_05_13_google_translator.pkl', 'rb') as f:\n",
    "    lr_model = pickle.load(f)\n",
    "\n",
    "sum_feature_cols_lr  = [feature for feature in ORIGINAL_SUM_FEATURE_COLS if feature in lr_model.feature_names_in_]\n",
    "mean_feature_cols_lr = [feature + '_mean' for feature in ORIGINAL_MEAN_FEATURE_COLS if feature + '_mean' in lr_model.feature_names_in_]\n",
    "readability_list_lr  = [feature for feature in ORIGINAL_REDABILITY_LIST if feature in lr_model.feature_names_in_]\n",
    "features_lr          = [sum_feature_cols_lr, mean_feature_cols_lr, readability_list_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Aux functions\n",
    "import typer\n",
    "import spacy\n",
    "empath = Empath()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "diccionario = {k:v for v,k in emoji_data_df.values if len(k.split(' ')) > 1}\n",
    "def emojize(texto):\n",
    "  # Paso 1: ordenar claves por longitud en orden descendente\n",
    "  claves = sorted(diccionario, key=len, reverse=True)\n",
    "\n",
    "  # Paso 2: reemplazar claves por valores en orden descendente\n",
    "  for clave in claves:\n",
    "      valor = diccionario[clave]\n",
    "      texto = texto.replace(clave, valor)\n",
    "\n",
    "  return texto.replace('signo de interrogación', '?').replace('signo de exclamación', '!')\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def get_empath_features(df):\n",
    "    for empath_ in ['suffering', 'violence', 'timidity', 'fear', 'body', 'youth', 'pain', 'sadness', 'nervousness', 'money']:\n",
    "        df[empath_] = df['text'].apply(lambda x: empath.analyze(lemmatizer.lemmatize(str(x)), categories=[empath_])[empath_])\n",
    "    return df\n",
    "\n",
    "# -- Function to get Part of Speech (POS) tags, using Spacy library (filtering by POS)\n",
    "def get_pos_tags(df, tag, colname):\n",
    "    pos = []\n",
    "    for doc in nlp.pipe(df['cleaned_text'].values, batch_size=16):\n",
    "        pos_list = [token.pos_ for token in doc if token.pos_ == tag]\n",
    "        pos.append(len(pos_list))\n",
    "    df[colname] = pos\n",
    "    return df\n",
    "\n",
    "# -- Function to get other text features\n",
    "def other_text_features(df):\n",
    "    # -- Punctuation signs\n",
    "    df[\"fe_punct_signs\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"[\\?¿!¡]\", str(x))))\n",
    "\n",
    "    # -- Get first person pronouns\n",
    "    forms = r\"|\".join(regex for regex in [r'\\bi\\b' , r'\\bmy+\\b', r'\\bme+\\b', r'\\bmine+\\b'])\n",
    "    df['fe_num_first_person_pronouns'] = df['text'].apply(lambda x: len(re.findall(forms, str(x).lower())))\n",
    "\n",
    "    # -- Get number of long words\n",
    "    readability_preprocessing = lambda x: '\\n\\n'.join('\\n'.join(' '.join(token.value for token in sentence) for sentence in paragraph) for paragraph in segmenter.analyze(x))\n",
    "    df['fe_num_long_words'] = df['text'].apply(lambda x: readability.getmeasures(readability_preprocessing(clean_text_no_lower(str(x))), lang='en')['sentence info']['long_words']\\\n",
    "                                                if re.search(r\"[a-zA-Z]\", clean_text_no_lower(str(x))) != None else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -- Function to get mood-problem terms\n",
    "def get_mood_terms(df):\n",
    "    ## Load negative forms\n",
    "    neg_forms = [\"rarely\",\"never\",\"no more\",\"regain control of\",\"not\",\"don[\\']t\",\"didn[\\']t\",\"wouldn[\\']t\",\"not very\",\"scarcely\",\"no longer\",\"hardly\",\"contradictorily\",\"invalidly\",\"seldom\",\"barely ever\",\"hardly ever\",\"under no circumstances\",\"in no way\",\"on no condition\",\"wasn[\\']t\",\"isn[\\']t\",\"was not\",\"is not\",\"aren[\\']t\",\"are not\",\"maybe\",\"stop\",\"stopped\",\"end of\",\"doesn[\\']t\",\"doesn not\",\"couldn[\\']t\",\"could not\",\"can[\\']t\",\"can not\",\"cannot\",\"pretend to\",\"feel no\"]\n",
    "    neg_forms_joined = \"\".join(\"(?<!\" + neg_regex + \")\" for neg_regex in neg_forms)\n",
    "    ## Sample with mood\n",
    "    mood_df = pd.read_table('mood_problem.txt', header=None)\n",
    "    mood_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # -- but these past few weeks I've had random crying phases that pop up several times a day; I feel completely alone; \n",
    "    mood_list = list(mood_df[0].apply(lambda x: x.replace(\"(\", \"(?:\").replace(\"( )\", \"\").replace(\"?:?<!\", \"?<!\").replace(\"?:?!\", \"?!\").replace(\"(?: )\", \"\").replace(\"#\", \"\").replace(\"?(\\w+)?\", \"?(?:[a-zA-Z ]*)\").replace('’', \"'\").lower()))\n",
    "    mood_list.extend([\"i\\'?m so+ sad\", \"i’?\\'?m so+ depressed\", \"i\\'?m so+ unhappy\", \"i cry\", \"am( so+)? sad\", r\"my life\\'s (pretty much|like) a roller coaster with( major)? ups and downs\", r\"anti\\-depressant\",\n",
    "                    r\"it tears me up\", r\"i just break down\", r\"i( just)? feel emotionless\", r\"i wish i could find some magical cure that can make me happy but i don\\'t think there is one\",\n",
    "                    r\"i even had a major mental breakdown at work where all my co\\-workers saw me cry\",\n",
    "                    r\"i[\\' ](m|am) so tired of being so sad\", r\"i[\\' ](have|ve) noticed moodiness\", r\"i[\\' ](m|am)( feeling| just| still| so| stuck in this| fucking| stuck and| feel just as| feel)? miserable\",\n",
    "                    r\"i spent most of the rest of the day feeling miserable\", r\"i can(\\'t|not) stop crying\", \"i feel very sad\", \"i feel a mix of anger and sadness\", \"i sat downstairs alone at night crying to myself\",\n",
    "                    r\"i\\'?m alone\", r\"i( just)? feel( so)? alone\", r\"i\\'ve had a relapse of sadness\", r\"i feel like i have no one\", r\"i can\\'t feel happy\", r\"i\\'?( am|m) (at|in) the end of my rope\", r\"i am sitting here with tears\",\n",
    "                    r\"i feel totally wrong and get emotional when i get back home, crying and anxious, the whole bit\", r\"i\\'ve find myself crying when the memory hits\"])\n",
    "    mod_list = [regex for regex in mood_list if \"depress\" not in regex]\n",
    "    df['fe_num_mood_terms_aux'] = df['text'].apply(lambda x: sum([len(list(set(re.findall(term, str(x).lower().replace('’', \"'\"))))) \\\n",
    "                                                                                                                    for term in mood_list])\n",
    "                                                                                                        )\n",
    "    # -- Apply to mood\n",
    "    mood_list = list(mood_df[0].apply(lambda x: neg_forms_joined + \" ?\" + x.replace(\"(\", \"(?:\").replace(\"( )\", \"\").replace(\"?:?<!\", \"?<!\").replace(\"?:?!\", \"?!\").replace(\"(?: )\", \"\").replace(\"#\", \"\").replace(\"?(\\w+)?\", \"?(?:[a-zA-Z ]*)\").replace('’', \"'\").lower()))\n",
    "    df['fe_num_mood_terms'] = df.apply(lambda x: sum([len(list(set(re.findall(term, str(x['text']).lower().replace('’', \"'\"))))) \\\n",
    "                                                                                                   for term in mood_list])\\\n",
    "                                                                                                        if x['fe_num_mood_terms_aux'] > 0 else 0,\n",
    "                                                                                                    axis=1\n",
    "                                                                                            )\n",
    "    df.drop(['fe_num_mood_terms_aux'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# -- Function to preprocess text (based on HugginFace transformer)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if 'http' in t else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# -- Function to get RoBERTa base sentiment + VADER\n",
    "def get_sentiment_features(df):\n",
    "    with torch.no_grad():\n",
    "        df[\"fe_sentiment\"] = df[\"text\"].progress_apply(lambda x: pipe_sentiment(preprocess(str(x).replace(\"\\n\", \" \").replace(\"\\t\", \" \"))))\n",
    "    emotions = ['negative', 'neutral', 'positive']\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        df['fe_roberta_base_sentiment_' + emotion + '_mean'] =  df['fe_sentiment'].apply(lambda x: x[i]['score'])\n",
    "    df.drop([\"fe_sentiment\"], axis=1, inplace=True)\n",
    "\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    df['fe_vader_positive_sentiment_mean'] = df['text'].apply(lambda x: sid_obj.polarity_scores(clean_text_no_lower(str(x)))['pos'])\n",
    "    df['fe_vader_neutral_sentiment_mean']  = df['text'].apply(lambda x: sid_obj.polarity_scores(clean_text_no_lower(str(x)))['neu'])\n",
    "    df['fe_vader_negative_sentiment_mean'] = df['text'].apply(lambda x: sid_obj.polarity_scores(clean_text_no_lower(str(x)))['neg'])\n",
    "    \n",
    "    # -- Emotion features\n",
    "    emotion_list = ['optimism', 'joy', 'sadness']\n",
    "    with torch.no_grad():\n",
    "        df[\"fe_distilbert_emotion\"] = df[\"text\"].apply(lambda x: pipe_emotion(preprocess(str(x).replace(\"\\n\", \" \").replace(\"\\t\", \" \")), max_length=511))\n",
    "    \n",
    "    for i, emotion in enumerate(emotion_list):\n",
    "        df['fe_distilbert_emotion_' + emotion + '_mean'] =  df['fe_distilbert_emotion'].apply(lambda x: x[i]['score'])\n",
    "\n",
    "    df.drop('fe_distilbert_emotion', axis=1, inplace=True)\n",
    "    \n",
    "    # -- NRCLex emotion\n",
    "    df['fe_nrclex_emotions'] = df['text'].apply(lambda x: NRCLex(clean_text_no_lower(str(x)).replace(\"\\n\", \" \").replace(\"\\t\", \" \")).affect_frequencies)\n",
    "    \n",
    "    for emotion in ['surprise', 'positive', 'sadness', 'joy']:\n",
    "        df['fe_nrclex_emotion_' + emotion + '_mean'] = df['fe_nrclex_emotions'].apply(lambda x: x[emotion])\n",
    "    df.drop('fe_nrclex_emotions', axis=1, inplace=True)\n",
    "    \n",
    "    # -- Toxicity\n",
    "    with torch.no_grad():\n",
    "        df[\"fe_toxicity\"] = df[\"text\"].apply(lambda x: pipe_toxicity(preprocess(str(x).replace(\"\\n\", \" \").replace(\"\\t\", \" \"))))\n",
    "\n",
    "    df['fe_toxic_mean'] = df['fe_toxicity'].apply(lambda x: x[0]['score'])\n",
    "    df['fe_insult_mean'] = df['fe_toxicity'].apply(lambda x: x[4]['score'])\n",
    "    df['fe_identity_hate_mean'] = df['fe_toxicity'].apply(lambda x: x[5]['score'])\n",
    "    df.drop(['fe_toxicity'], axis=1, inplace=True)\n",
    "    \n",
    "    # -- Polarity\n",
    "    df[\"fe_polarity_mean\"] = df[\"text\"].apply(lambda x: TextBlob(clean_text_no_lower(str(x))).sentiment.polarity)\n",
    "    return df\n",
    "\n",
    "def get_readability_features(df):\n",
    "    # 'fe_coleman_readability_index', 'fe_smog_index'\n",
    "    readability_preprocessing = lambda x: '\\n\\n'.join('\\n'.join(' '.join(token.value for token in sentence) for sentence in paragraph) for paragraph in segmenter.analyze(x))\n",
    "    df['fe_coleman_readability_index'] = df['text'].apply(lambda x: readability.getmeasures(readability_preprocessing(clean_text_no_lower(str(x))), lang='en')['readability grades']['Coleman-Liau'] if re.search(r\"[a-zA-Z]\", clean_text_no_lower(str(x))) != None else 0)\n",
    "    df['fe_smog_index'] = df['text'].apply(lambda x: readability.getmeasures(readability_preprocessing(clean_text_no_lower(str(x))), lang='en')['readability grades']['SMOGIndex'] if re.search(r\"[a-zA-Z]\", clean_text_no_lower(str(x))) != None else 0)\n",
    "    return df\n",
    "\n",
    "# -- Function to remove URLs from text\n",
    "def clean_text_no_lower(x):\n",
    "    return re.sub(r\"(#[a-zA-Z0-9]+;)|(\\/r\\/[a-zA-Z]+)|([a-zA-Z%_0-9]+=[a-zA-Z0-9%&_\\.-]* ?\\)?\\]?|(%[a-zA-Z0-9_\\.=,\\-\\+%]+(watch|facebook|reddit|http|youtube\\.)[a-zA-Z0-9_\\.=,\\-%\\+]+( \\+O%27Nymous)?\\)?\\]?))|(!\\/?[a-zA-Z0-9.\\/_]+\\.[a-zA-Z]{2,3}\\)?\\]?)\", \"\",\n",
    "                  re.sub(r'\\(?\\[?(?:(?:http|https|www)\\:*(\\/+\\/+\\**|\\.))[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#\\*\\\\\\.]+\\.((?:[a-zA-Z]){2,6}(?:[a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#\\*\\\\\\.])*)?\\)?\\]?', '', \n",
    "                  str(re.sub(' +', ' ', str(x))), flags=re.MULTILINE))\n",
    "\n",
    "# -- Function (general) to clean texts (removing URLs, hashtags, punctuation sings, words with less than 2 chars) + lowercase\n",
    "def clean_text(x):\n",
    "    x = str(x).lower()\n",
    "    x = re.sub(r\"(#[a-zA-Z0-9]+;)|(\\/r\\/[a-zA-Z]+)|([a-zA-Z%_0-9]+=[a-zA-Z0-9%&_\\.-]* ?\\)?\\]?|(%[a-zA-Z0-9_\\.=,\\-\\+%]+(watch|facebook|reddit|http|youtube\\.)[a-zA-Z0-9_\\.=,\\-%\\+]+( \\+O%27Nymous)?\\)?\\]?))|(!\\/?[a-zA-Z0-9.\\/_]+\\.[a-zA-Z]{2,3}\\)?\\]?)\", \"\",\n",
    "                  re.sub(r'\\(?\\[?(?:(?:http|https|www)\\:*(\\/+\\/+\\**|\\.))[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#\\*\\\\\\.]+\\.((?:[a-zA-Z]){2,6}(?:[a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#\\*\\\\\\.])*)?\\)?\\]?', '', \n",
    "                  str(re.sub(' +', ' ', str(x))), flags=re.MULTILINE))\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WgHNiyxHR5AJ"
   },
   "source": [
    "# Download Data\n",
    "To download the data, you can make use of the **functions defined in the following**.\n",
    "\n",
    "The following function download the trial data. To adapt it to download the train and test data, follow the instructions given in the [website of the competition](https://sites.google.com/view/mentalriskes/evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uaeh23C5R1lG"
   },
   "outputs": [],
   "source": [
    "def download_messages_trial(task: str,subtasks:List[str], token: str) -> List[Dict]:\n",
    "    response = requests.get(ENDPOINT_DOWNLOAD_MESSAGES_TRIAL.format(TASK=task, TOKEN=token))\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Trial - Status Code \" + task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
    "    else:\n",
    "      z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "      os.makedirs(\"./data/{task}/trial/subjects_trial/\".format(task=task))\n",
    "      z.extractall(\"./data/{task}/trial/subjects_trial/\".format(task=task))\n",
    "\n",
    "    for subtask in subtasks:\n",
    "        response = requests.get(ENDPOINT_DOWNLOAD_GOLD_TRIAL.format(SUBTASK=subtask, TOKEN=token))\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(\"Trial - Status Code \" + subtask + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
    "        else:\n",
    "          file_object = open(\"./data/{task}/trial/gold_trial_{subtask}.txt\".format(task=task, subtask=subtask), \"w\")\n",
    "          file_object.write(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VIqRCv3OS3Bn"
   },
   "source": [
    "# Client Server\n",
    "This class simulates communication with our server. The following code established the conection with the server client and simulate the GET and POST requests. \n",
    "\n",
    "**IMPORTANT NOTE:** Please pay attention to the basic functions and remember that it is only a base for your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "l0kONpltS2R9"
   },
   "outputs": [],
   "source": [
    "class Client_taskX:\n",
    "    def __init__(self, task: str, subtasks: List[str], token: str, number_of_runs: int, tracker: EmissionsTracker):\n",
    "        # Task in which you participate\n",
    "        self.task = task\n",
    "        # Subtasks in which you participate\n",
    "        self.subtasks = subtasks\n",
    "        # Token identifier\n",
    "        self.token = token\n",
    "        # Number of runs (Max: 3)\n",
    "        self.number_of_runs = number_of_runs\n",
    "        # Object to calculate CO2 emissions\n",
    "        self.tracker = tracker\n",
    "        self.relevant_cols = ['duration', 'emissions', 'cpu_energy', 'gpu_energy', 'ram_energy', \n",
    "            'energy_consumed', 'cpu_count', 'gpu_count', 'cpu_model', 'gpu_model', 'ram_total_size']\n",
    "\n",
    "    # Here a GET request is sent to the server to extract the data.\n",
    "    def get_messages(self, retries: int, backoff: float) -> Dict:\n",
    "        session = requests.Session()\n",
    "        retries = Retry( \n",
    "                        total = retries,\n",
    "                        backoff_factor = backoff,\n",
    "                        status_forcelist = [500, 502, 503, 504]\n",
    "                        )\n",
    "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        response = session.get(ENDPOINT_GET_MESSAGES_TRIAL.format(TASK=self.task, TOKEN=self.token))\n",
    "        if response.status_code != 200:\n",
    "          print(\"GET - Status Code \" + self.task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
    "          return []\n",
    "        else:\n",
    "          return json.loads(response.content)\n",
    "\n",
    "    # The POST requests are sent to the server to send predictions and carbon emission data\n",
    "    def submit_decission(self, run: int, subtask: int, decisions: Dict, emissions:Dict, retries, backoff):\n",
    "\n",
    "        data = {\n",
    "            \"predictions\": decisions,\n",
    "            \"emissions\": emissions\n",
    "        }\n",
    "\n",
    "        data = json.dumps(data)\n",
    "        ## Session to POST request\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "                        total = retries,\n",
    "                        backoff_factor = backoff,\n",
    "                        status_forcelist = [500, 502, 503, 504]\n",
    "                        )\n",
    "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        response = session.post(ENDPOINT_SUBMIT_DECISIONS.format(SUBTASK=self.subtasks[subtask], TOKEN=self.token, RUN=run), json=[data])\n",
    "        if response.status_code != 200:\n",
    "            print(\"POST - Status Code \" + self.task + \": \" + str(response.status_code) + \" - Error: \" + str(response.text))\n",
    "        else:\n",
    "            print(\"Subtask {}: - run {}\".format(self.subtasks[subtask], run))\n",
    "\n",
    "    # Main thread\n",
    "    def run_taskX(self, retries: int, backoff: float):\n",
    "        # -- Load googletranslator\n",
    "        translator = Translator()\n",
    "\n",
    "        # Get messages for taskX\n",
    "        messages = self.get_messages(retries, backoff)\n",
    "        #messages = [{\"id_message\": 123,\"round\": 1,\"nick\": \"subject1\",\"message\": \"Hola a todos! La verdad es que me alegro de estar aquí!\",\"date\": \"2021/02/10\"},\n",
    "        #            {\"id_message\": 124,\"round\": 1,\"nick\": \"subject1\",\"message\": \"Adiós a todos! Ha sido un verdadero placer! cara llorando\",\"date\": \"2021/02/10\"},\n",
    "        #            {\"id_message\": 125,\"round\": 1,\"nick\": \"subject2\",\"message\": \"Adiós a todos! Ha sido un verdadero placer! cara llorando\",\"date\": \"2021/02/10\"}]\n",
    "\n",
    "        # If there are no messages\n",
    "        if len(messages) == 0:\n",
    "            print(\"All rounds processed\")\n",
    "            return\n",
    "\n",
    "        # -- Main while loop\n",
    "        all_messages = pd.DataFrame(columns=['id_message', 'round', 'nick', 'message'])\n",
    "        while len(messages) > 0:\n",
    "            print(\"------------------- Processing round {}\".format(messages[0][\"round\"]))\n",
    "            # Save subjects\n",
    "            with open('./data/rounds_trial/round{}.json'.format(messages[0][\"round\"]), 'w+', encoding='utf8') as json_file:\n",
    "                json.dump(messages, json_file, ensure_ascii=False)\n",
    "                            \n",
    "            self.tracker.start()\n",
    "            # -- Step 1: convert dict to Pandas DataFrame\n",
    "            messages_df[\"message_emojized\"]       = messages_df[\"message\"].apply(emojize)\n",
    "            messages_df['message_without_emojis'] = messages_df['message_emojized'].apply(lambda x: remove_emojis(x))\n",
    "\n",
    "            # -- Step 2: translate it using googletrans library\n",
    "            messages_df[\"text\"] = messages_df[\"message_without_emojis\"].apply(lambda x: translator.translate(str(x), dest=\"en\").text)\n",
    "\n",
    "            # -- Step 3: feature engine\n",
    "            messages_df = get_empath_features(messages_df)\n",
    "\n",
    "            # -- Step 4: POS features - adverbs\n",
    "            messages_df['cleaned_text'] = messages_df['text'].apply(clean_text_no_lower)\n",
    "            messages_df = get_pos_tags(messages_df, 'ADV', 'fe_pos_advs')\n",
    "\n",
    "            # -- Step 5: Get other text features\n",
    "            messages_df = other_text_features(messages_df)\n",
    "\n",
    "            # -- Step 6: Get mood problems terms\n",
    "            messages_df = get_mood_terms(messages_df)\n",
    "\n",
    "            # -- Step 7: Get sentiment + toxicity + emotion features\n",
    "            messages_df = get_sentiment_features(messages_df)\n",
    "\n",
    "            # -- Step 8: Get readability features\n",
    "            messages_df = get_readability_features(messages_df)\n",
    "            \n",
    "            # -- Concat with all_messages\n",
    "            all_messages = pd.concat([all_messages, messages_df], axis=0)\n",
    "            \n",
    "            if not all_messages.empty():\n",
    "                messages_df = pd.concat([messages_df, all_messages[all_messages['nick'].isin(messages_df['nick'])]],\n",
    "                                        axis=0)\n",
    "\n",
    "            for i, (model, model_features) in enumerate(zip([lightgbm_model, rf_model, lr_model],\n",
    "                                                            [features_lightgbm, features_rf, features_lr])):\n",
    "                \n",
    "                \n",
    "                messages_df_grouped = messages_df.groupby(['nick'])[model_features[0]].apply(lambda x : x.astype(int).sum()).reset_index().merge(\n",
    "                                                                    messages_df.groupby(['nick'])[model_features[2]].apply(lambda x : x.mean()).reset_index(),\n",
    "                                                                    on=['nick']\n",
    "                                                            ).merge(\n",
    "                                                                messages_df.groupby(['nick'])[model_features[1]].apply(lambda x : x.mean()).reset_index(),\n",
    "                                                                on=['nick']\n",
    "                                                            )\n",
    "                \n",
    "                ## -- Calculate prediction\n",
    "                if type(model).__name__ in ['RandomForestClassifier', 'LogisticRegression']:\n",
    "                    predictions = model.predict(messages_df_grouped[model.feature_names_in_])\n",
    "                else:\n",
    "                    predictions = model.predict(messages_df_grouped[model.feature_name_])\n",
    "                decissions  = {id: int(pred) for id, pred in zip(messages_df_grouped['nick'], predictions)}\n",
    "\n",
    "                emissions = self.tracker.stop()\n",
    "                emissions_df = pd.read_csv(\"emissions.csv\")\n",
    "                measurements = emissions_df.iloc[-1][self.relevant_cols].to_dict()\n",
    "                self.submit_decission(subtask=0, run=i, decisions=decissions, emissions=measurements, retries=retries, backoff=backoff)\n",
    "                time.sleep(3)\n",
    "        print(\"All rounds processed\")\n",
    "        return decissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = pd.DataFrame(columns=['id_message', 'round', 'nick', 'message'])\n",
    "all_messages.empty()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gMXuHLciXIO3"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KvhdFtEajwmp"
   },
   "source": [
    "Please, replace the symbol 'X' by the desired task. For example, for task 1 it would be: task1, task1a and task1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:33:15] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:33:15] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:33:15] No GPU found.\n",
      "[codecarbon INFO @ 14:33:15] [setup] CPU Tracking...\n",
      "[codecarbon DEBUG @ 14:33:15] Not using PowerGadget, an exception occurred while instantiating IntelPowerGadget : Intel Power Gadget executable not found on darwin\n",
      "[codecarbon DEBUG @ 14:33:15] Not using the RAPL interface, an exception occurred while instantiating IntelRAPL : Platform not supported by Intel RAPL Interface\n",
      "[codecarbon WARNING @ 14:33:15] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 14:33:17] We saw that you have a Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 14:33:17] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 14:33:17] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:33:17]   Platform system: macOS-10.16-x86_64-i386-64bit\n",
      "[codecarbon INFO @ 14:33:17]   Python version: 3.8.3\n",
      "[codecarbon INFO @ 14:33:17]   CodeCarbon version: 2.2.1\n",
      "[codecarbon INFO @ 14:33:17]   Available RAM : 8.000 GB\n",
      "[codecarbon INFO @ 14:33:17]   CPU count: 4\n",
      "[codecarbon INFO @ 14:33:17]   CPU model: Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 14:33:17]   GPU count: None\n",
      "[codecarbon INFO @ 14:33:17]   GPU model: None\n",
      "[codecarbon DEBUG @ 14:33:18] Not running on AWS\n",
      "[codecarbon DEBUG @ 14:33:19] Not running on Azure\n",
      "[codecarbon DEBUG @ 14:33:20] Not running on GCP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Processing round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.33it/s]\n",
      "[codecarbon INFO @ 14:33:21] Energy consumed for RAM : 0.000000 kWh. RAM Power : 0.5084638595581055 W\n",
      "[codecarbon DEBUG @ 14:33:21] RAM : 0.51 W during 1.13 s [measurement time: 0.0118]\n",
      "[codecarbon INFO @ 14:33:21] Energy consumed for all CPUs : 0.000014 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon DEBUG @ 14:33:21] CPU : 42.50 W during 1.14 s [measurement time: 0.0008]\n",
      "[codecarbon INFO @ 14:33:21] 0.000014 kWh of electricity used since the beginning.\n",
      "[codecarbon DEBUG @ 14:33:21] last_duration=1.144747018814087\n",
      "------------------------\n",
      "[codecarbon DEBUG @ 14:33:21] We apply an energy mix of 190 g.CO2eq/kWh for Spain\n",
      "[codecarbon DEBUG @ 14:33:21] EmissionsData(timestamp='2023-05-20T14:33:21', project_name='codecarbon', run_id='f944fe69-1dca-479c-a1dd-3186000aec01', duration=1.158128023147583, emissions=2.5980071092583097e-06, emissions_rate=2.243281448451092e-06, cpu_power=42.5, gpu_power=0.0, ram_power=0.5084638595581055, cpu_energy=1.3514374527666304e-05, gpu_energy=0, ram_energy=1.5934710000901002e-07, energy_consumed=1.3673721627675314e-05, country_name='Spain', country_iso_code='ESP', region='madrid', cloud_provider='', cloud_region='', os='macOS-10.16-x86_64-i386-64bit', python_version='3.8.3', codecarbon_version='2.2.1', cpu_count=4, cpu_model='Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz', gpu_count=None, gpu_model=None, longitude=-3.4719, latitude=40.4586, ram_total_size=8.0, tracking_mode='process', on_cloud='N')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": {\"subject1\": 0, \"subject2\": 0}, \"emissions\": {\"duration\": 1.158128023147583, \"emissions\": 2.5980071092583097e-06, \"cpu_energy\": 1.3514374527666304e-05, \"gpu_energy\": 0, \"ram_energy\": 1.5934710000901004e-07, \"energy_consumed\": 1.3673721627675314e-05, \"cpu_count\": 4, \"gpu_count\": NaN, \"cpu_model\": \"Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz\", \"gpu_model\": NaN, \"ram_total_size\": 8.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:33:24] Tracker already stopped !\n",
      "[codecarbon DEBUG @ 14:33:24] We apply an energy mix of 190 g.CO2eq/kWh for Spain\n",
      "[codecarbon DEBUG @ 14:33:24] EmissionsData(timestamp='2023-05-20T14:33:24', project_name='codecarbon', run_id='f944fe69-1dca-479c-a1dd-3186000aec01', duration=4.277153015136719, emissions=2.5980071092583097e-06, emissions_rate=6.074150492311215e-07, cpu_power=42.5, gpu_power=0.0, ram_power=0.5084638595581055, cpu_energy=1.3514374527666304e-05, gpu_energy=0, ram_energy=1.5934710000901002e-07, energy_consumed=1.3673721627675314e-05, country_name='Spain', country_iso_code='ESP', region='madrid', cloud_provider='', cloud_region='', os='macOS-10.16-x86_64-i386-64bit', python_version='3.8.3', codecarbon_version='2.2.1', cpu_count=4, cpu_model='Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz', gpu_count=None, gpu_model=None, longitude=-3.4719, latitude=40.4586, ram_total_size=8.0, tracking_mode='process', on_cloud='N')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": {\"subject1\": 0, \"subject2\": 0}, \"emissions\": {\"duration\": 4.277153015136719, \"emissions\": 2.5980071092583097e-06, \"cpu_energy\": 1.3514374527666304e-05, \"gpu_energy\": 0, \"ram_energy\": 1.5934710000901004e-07, \"energy_consumed\": 1.3673721627675314e-05, \"cpu_count\": 4, \"gpu_count\": NaN, \"cpu_model\": \"Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz\", \"gpu_model\": NaN, \"ram_total_size\": 8.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:33:27] Tracker already stopped !\n",
      "[codecarbon DEBUG @ 14:33:27] We apply an energy mix of 190 g.CO2eq/kWh for Spain\n",
      "[codecarbon DEBUG @ 14:33:27] EmissionsData(timestamp='2023-05-20T14:33:27', project_name='codecarbon', run_id='f944fe69-1dca-479c-a1dd-3186000aec01', duration=7.3230791091918945, emissions=2.5980071092583097e-06, emissions_rate=3.547697724577771e-07, cpu_power=42.5, gpu_power=0.0, ram_power=0.5084638595581055, cpu_energy=1.3514374527666304e-05, gpu_energy=0, ram_energy=1.5934710000901002e-07, energy_consumed=1.3673721627675314e-05, country_name='Spain', country_iso_code='ESP', region='madrid', cloud_provider='', cloud_region='', os='macOS-10.16-x86_64-i386-64bit', python_version='3.8.3', codecarbon_version='2.2.1', cpu_count=4, cpu_model='Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz', gpu_count=None, gpu_model=None, longitude=-3.4719, latitude=40.4586, ram_total_size=8.0, tracking_mode='process', on_cloud='N')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": {\"subject1\": 0, \"subject2\": 0}, \"emissions\": {\"duration\": 7.3230791091918945, \"emissions\": 2.5980071092583097e-06, \"cpu_energy\": 1.3514374527666304e-05, \"gpu_energy\": 0, \"ram_energy\": 1.5934710000901004e-07, \"energy_consumed\": 1.3673721627675314e-05, \"cpu_count\": 4, \"gpu_count\": NaN, \"cpu_model\": \"Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz\", \"gpu_model\": NaN, \"ram_total_size\": 8.0}}\n",
      "All rounds processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject1': 0, 'subject2': 0}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker = EmissionsTracker(**config)\n",
    "\n",
    "number_runs = 3 # Max: 3\n",
    "\n",
    "# Prediction period\n",
    "client_taskX = Client_taskX(\"task2\", [\"task2a\"], TOKEN, number_runs, tracker)\n",
    "client_taskX.run_taskX(5, 0.1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
